{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPPPQIwu54VzyTjIBvNXhEM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hdBRfKB-lBE5","executionInfo":{"status":"ok","timestamp":1752575096040,"user_tz":-330,"elapsed":554,"user":{"displayName":"Geetanjali Sharma","userId":"06177037883473308767"}},"outputId":"c77dea4e-3403-4030-9a16-344435631453"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3-2045477963.py:127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n","  states_tensor = torch.FloatTensor(episode_states)\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1, Return: 38.00\n","Episode 2, Return: 17.00\n","Episode 3, Return: 17.00\n","Episode 4, Return: 26.00\n","Episode 5, Return: 12.00\n","Episode 6, Return: 14.00\n","Episode 7, Return: 13.00\n","Episode 8, Return: 46.00\n","Episode 9, Return: 39.00\n","Episode 10, Return: 28.00\n"]}],"source":["# ----------------------------\n","# Import required libraries\n","# ----------------------------\n","import numpy as np\n","np.bool8 = np.bool_  # Fix for NumPy >= 1.24\n","\n","import torch  # For building and training neural networks\n","import torch.nn as nn\n","import torch.optim as optim\n","import gym  # OpenAI Gym for RL environments\n","\n","# ----------------------------\n","# Create the CartPole environment\n","# ----------------------------\n","env = gym.make(\"CartPole-v1\")\n","\n","# ----------------------------\n","# Set random seeds for reproducibility\n","# ----------------------------\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# ----------------------------\n","# Get state and action space sizes\n","# ----------------------------\n","state_dim = env.observation_space.shape[0]  # 4 for CartPole\n","num_actions = env.action_space.n            # 2 actions: left or right\n","\n","# ----------------------------\n","# Define the policy network π(a|s)\n","# ----------------------------\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(PolicyNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, 32)      # Hidden layer with 32 neurons\n","        self.relu = nn.ReLU()                    # ReLU activation\n","        self.fc2 = nn.Linear(32, output_dim)     # Output layer\n","        self.softmax = nn.Softmax(dim=-1)        # Action probabilities\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        logits = self.fc2(x)\n","        probs = self.softmax(logits)\n","        return probs\n","\n","# ----------------------------\n","# Initialize network and optimizer\n","# ----------------------------\n","policy_net = PolicyNetwork(state_dim, num_actions)\n","optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n","\n","# ----------------------------\n","# Reward-to-go function\n","# ----------------------------\n","def compute_reward_to_go(rewards, gamma=0.99):\n","    \"\"\"\n","    Compute reward-to-go for each timestep.\n","    G_t = R_t + γR_{t+1} + γ²R_{t+2} + ...\n","    \"\"\"\n","    rtg = np.zeros_like(rewards, dtype=np.float32)\n","    running_add = 0\n","    for t in reversed(range(len(rewards))):\n","        running_add = rewards[t] + gamma * running_add\n","        rtg[t] = running_add\n","    return rtg\n","\n","# ----------------------------\n","# Number of training episodes\n","# ----------------------------\n","num_episodes = 10  # Increase to 500+ for better training\n","\n","# ----------------------------\n","# Training loop\n","# ----------------------------\n","for episode in range(num_episodes):\n","    # Reset environment (handle both new and old Gym versions)\n","    reset_output = env.reset()\n","    if isinstance(reset_output, tuple):\n","        state, _ = reset_output  # New Gym API\n","    else:\n","        state = reset_output     # Old Gym API\n","\n","    done = False  # Whether episode is complete\n","\n","    episode_states = []   # List of states\n","    episode_actions = []  # List of actions\n","    episode_rewards = []  # List of rewards\n","\n","    # ----------------------------\n","    # Run one episode\n","    # ----------------------------\n","    while not done:\n","        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # shape: [1, state_dim]\n","\n","        # Get action probabilities from the policy\n","        action_probs = policy_net(state_tensor).detach().numpy().ravel()\n","\n","        # Sample action from the probability distribution\n","        action = np.random.choice(num_actions, p=action_probs)\n","\n","        # Take the action in the environment\n","        step_output = env.step(action)\n","        if len(step_output) == 5:  # New Gym API\n","            next_state, reward, terminated, truncated, _ = step_output  # gives you the reward at each timestep\n","            done = terminated or truncated\n","        else:  # Old Gym API\n","            next_state, reward, done, _ = step_output\n","\n","        # Save transition\n","        episode_states.append(state)\n","        episode_actions.append(action)\n","        episode_rewards.append(reward)\n","\n","        # Move to the next state\n","        state = next_state\n","\n","    # ----------------------------\n","    # Compute normalized reward-to-go\n","    # ----------------------------\n","    reward_to_go = compute_reward_to_go(episode_rewards)\n","    reward_to_go -= np.mean(reward_to_go)\n","    reward_to_go /= (np.std(reward_to_go) + 1e-8)\n","    reward_to_go_tensor = torch.FloatTensor(reward_to_go)\n","\n","    # Convert episode data to tensors\n","    states_tensor = torch.FloatTensor(episode_states)\n","    actions_tensor = torch.LongTensor(episode_actions)\n","\n","    # Get action probabilities for all states\n","    action_probs = policy_net(states_tensor)\n","\n","    # Get log-probabilities of the actions that were actually taken\n","    selected_log_probs = torch.log(action_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze())\n","\n","    # Compute policy gradient loss: REINFORCE with reward-to-go\n","    loss = -torch.mean(selected_log_probs * reward_to_go_tensor)\n","\n","    # Backpropagation step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # ----------------------------\n","    # Logging\n","    # ----------------------------\n","    print(f\"Episode {episode + 1}, Return: {np.sum(episode_rewards):.2f}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Wyq43586lgHS"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPEKcBSXgzfJ2uW0qARYIUL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":620},"id":"zVhjhhacdetc","executionInfo":{"status":"ok","timestamp":1753176708380,"user_tz":-330,"elapsed":14140,"user":{"displayName":"Geetanjali Sharma","userId":"06177037883473308767"}},"outputId":"19bcdec7-2887-4511-a89d-7ea37313b1a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.23.5\n","  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","bigframes 2.11.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","blosc2 3.6.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n","db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.24.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"15dd4c646b9d40f88d3c4b3d7a6c45d3"}},"metadata":{}}],"source":["!pip install numpy==1.23.5\n"]},{"cell_type":"code","source":["import gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","# Detect Gym version\n","import gym\n","from packaging import version\n","is_new_gym = version.parse(gym.__version__) >= version.parse(\"0.26.0\")\n","\n","# Hyperparameters\n","learning_rate = 3e-4\n","gamma = 0.99\n","num_episodes = 1000\n","\n","env = gym.make(\"CartPole-v1\")\n","\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","\n","# Actor Network :  network maps a state to action probabilities . how likely each action is in a given state.\n","class Actor(nn.Module):\n","    def __init__(self):\n","        super(Actor, self).__init__()\n","        self.policy = nn.Sequential(\n","            nn.Linear(state_dim, 128),      # Fully connected layer taking input state vector and producing 128 features\n","            nn.ReLU(),                       # \tActivation function introducing non-linearity\n","            nn.Linear(128, action_dim),      # Output layer that maps features to number of possible actions\n","            nn.Softmax(dim=-1)               # \tConverts raw outputs to a probability distribution over actions\n","        )\n","\n","    def forward(self, state):\n","        return self.policy(state)\n","\n","#  actor network processes an input (state) when you call actor(state).\n","# It runs the input through the self.policy layers and returns the action probabilities.\n","\n","# Critic Network : The Critic learns to estimate the value function:\n","#It outputs a single number representing how good the current state is (i.e., V(s)).\n","# This models the state value function:V(s; φ) → predicted expected return from this state.\n","\n","class Critic(nn.Module):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.value = nn.Sequential(\n","            nn.Linear(state_dim, 128),     # hidden layer: transforms state input into 128 features\n","            # The state is a 1D vector of 4 floating-point numbers, each representing the physical state of the cart and pole system.\n","            nn.ReLU(),                     # \tNon-linearity to help the network learn complex functions\n","            nn.Linear(128, 1)              #  returns a single scalar value for the input state\n","        )\n","\n","    def forward(self, state):\n","        return self.value(state)\n","#  Defines how the critic processes an input state to estimate V(s).\n","\n","\n","# Instantiate\n","actor = Actor()\n","critic = Critic()\n","actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)  #  optimizer (Adam)-  update the actor network's weights during training.\n","critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate) # separate optimizer for the critic network's\n","\n","#Actor is trained using policy gradient.\n","#Critic is trained using value regression (e.g., TD error, MSE loss).\n","\n","\n","\n","# Training Loop\n","for episode in range(num_episodes):\n","    # Compatible reset\n","    if is_new_gym:\n","        state, _ = env.reset()\n","    else:\n","        state = env.reset()\n","\n","    state = torch.FloatTensor(state)\n","    total_reward = 0\n","    done = False\n","\n","    while not done:\n","        # Select an action using the policy\n","        #  This is a stochastic policy using the output probabilities from the actor.\n","        probs = actor(state)  # Forward pass through the actor network to get action probabilities.\n","        dist = torch.distributions.Categorical(probs)\n","        action = dist.sample()\n","\n","        # Compatible step\n","        if is_new_gym:\n","            next_state, reward, terminated, truncated, _ = env.step(action.item()) # Take action, observe reward, and move to next state\n","            done = terminated or truncated\n","        else:\n","            next_state, reward, done, _ = env.step(action.item()) # Take action, observe reward, and move to next state\n","\n","        next_state = torch.FloatTensor(next_state)\n","\n","        value = critic(state)\n","        next_value = critic(next_state)\n","\n","        # Step 3.3: Compute the policy gradient\n","        advantage = reward + (1 - int(done)) * gamma * next_value.item() - value.item()\n","        actor_loss = -dist.log_prob(action) * advantage\n","\n","        # Step 4: Update actor using policy gradient\n","        # Uses gradient ascent via optimizer to update θ.\n","        actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        actor_optimizer.step()\n","\n","        # Step 5: Compute critic loss\n","        #This is the TD error-based critic loss.\n","        target_value = reward + (1 - int(done)) * gamma * next_value\n","        critic_loss = nn.functional.mse_loss(value, target_value.detach())\n","\n","        # Update critic using gradient descent\n","        critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        critic_optimizer.step()\n","\n","        state = next_state\n","        total_reward += reward\n","\n","    if (episode + 1) % 10 == 0:\n","        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VEBUCQLUdimn","executionInfo":{"status":"ok","timestamp":1753177851409,"user_tz":-330,"elapsed":171448,"user":{"displayName":"Geetanjali Sharma","userId":"06177037883473308767"}},"outputId":"13b2a777-2b47-40c0-b942-5418b3117ae9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["Episode 10, Total Reward: 27.0\n","Episode 20, Total Reward: 22.0\n","Episode 30, Total Reward: 9.0\n","Episode 40, Total Reward: 20.0\n","Episode 50, Total Reward: 10.0\n","Episode 60, Total Reward: 9.0\n","Episode 70, Total Reward: 9.0\n","Episode 80, Total Reward: 22.0\n","Episode 90, Total Reward: 29.0\n","Episode 100, Total Reward: 11.0\n","Episode 110, Total Reward: 14.0\n","Episode 120, Total Reward: 18.0\n","Episode 130, Total Reward: 42.0\n","Episode 140, Total Reward: 19.0\n","Episode 150, Total Reward: 13.0\n","Episode 160, Total Reward: 15.0\n","Episode 170, Total Reward: 33.0\n","Episode 180, Total Reward: 10.0\n","Episode 190, Total Reward: 9.0\n","Episode 200, Total Reward: 13.0\n","Episode 210, Total Reward: 19.0\n","Episode 220, Total Reward: 10.0\n","Episode 230, Total Reward: 19.0\n","Episode 240, Total Reward: 15.0\n","Episode 250, Total Reward: 18.0\n","Episode 260, Total Reward: 13.0\n","Episode 270, Total Reward: 23.0\n","Episode 280, Total Reward: 10.0\n","Episode 290, Total Reward: 13.0\n","Episode 300, Total Reward: 29.0\n","Episode 310, Total Reward: 43.0\n","Episode 320, Total Reward: 29.0\n","Episode 330, Total Reward: 12.0\n","Episode 340, Total Reward: 25.0\n","Episode 350, Total Reward: 35.0\n","Episode 360, Total Reward: 27.0\n","Episode 370, Total Reward: 17.0\n","Episode 380, Total Reward: 37.0\n","Episode 390, Total Reward: 69.0\n","Episode 400, Total Reward: 63.0\n","Episode 410, Total Reward: 40.0\n","Episode 420, Total Reward: 30.0\n","Episode 430, Total Reward: 18.0\n","Episode 440, Total Reward: 38.0\n","Episode 450, Total Reward: 29.0\n","Episode 460, Total Reward: 113.0\n","Episode 470, Total Reward: 27.0\n","Episode 480, Total Reward: 48.0\n","Episode 490, Total Reward: 92.0\n","Episode 500, Total Reward: 64.0\n","Episode 510, Total Reward: 78.0\n","Episode 520, Total Reward: 68.0\n","Episode 530, Total Reward: 49.0\n","Episode 540, Total Reward: 119.0\n","Episode 550, Total Reward: 59.0\n","Episode 560, Total Reward: 36.0\n","Episode 570, Total Reward: 58.0\n","Episode 580, Total Reward: 112.0\n","Episode 590, Total Reward: 33.0\n","Episode 600, Total Reward: 27.0\n","Episode 610, Total Reward: 49.0\n","Episode 620, Total Reward: 23.0\n","Episode 630, Total Reward: 63.0\n","Episode 640, Total Reward: 57.0\n","Episode 650, Total Reward: 51.0\n","Episode 660, Total Reward: 67.0\n","Episode 670, Total Reward: 137.0\n","Episode 680, Total Reward: 119.0\n","Episode 690, Total Reward: 144.0\n","Episode 700, Total Reward: 125.0\n","Episode 710, Total Reward: 119.0\n","Episode 720, Total Reward: 94.0\n","Episode 730, Total Reward: 165.0\n","Episode 740, Total Reward: 83.0\n","Episode 750, Total Reward: 41.0\n","Episode 760, Total Reward: 23.0\n","Episode 770, Total Reward: 42.0\n","Episode 780, Total Reward: 21.0\n","Episode 790, Total Reward: 38.0\n","Episode 800, Total Reward: 48.0\n","Episode 810, Total Reward: 100.0\n","Episode 820, Total Reward: 99.0\n","Episode 830, Total Reward: 180.0\n","Episode 840, Total Reward: 278.0\n","Episode 850, Total Reward: 485.0\n","Episode 860, Total Reward: 126.0\n","Episode 870, Total Reward: 106.0\n","Episode 880, Total Reward: 33.0\n","Episode 890, Total Reward: 39.0\n","Episode 900, Total Reward: 34.0\n","Episode 910, Total Reward: 138.0\n","Episode 920, Total Reward: 169.0\n","Episode 930, Total Reward: 117.0\n","Episode 940, Total Reward: 101.0\n","Episode 950, Total Reward: 36.0\n","Episode 960, Total Reward: 46.0\n","Episode 970, Total Reward: 120.0\n","Episode 980, Total Reward: 79.0\n","Episode 990, Total Reward: 47.0\n","Episode 1000, Total Reward: 83.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kfZ4-vuxdipF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PKSyv_VSdisE"},"execution_count":null,"outputs":[]}]}
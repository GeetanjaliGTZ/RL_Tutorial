{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/I9utI7/41zXo1E8KMZ/V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import NumPy for numerical computations\n","import numpy as np\n","\n","# Import PyTorch modules\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# Import OpenAI Gym for reinforcement learning environments\n","import gym\n","\n","# Create the CartPole environment\n","env = gym.make(\"CartPole-v1\")\n","\n","# Set random seeds for reproducibility (so that results are consistent)\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Get the dimensionality of the state (CartPole has 4 inputs: position, velocity, angle, angular velocity))\n","state_dim = env.observation_space.shape[0]\n","\n","# Get the number of possible actions (CartPole has 2: left or right)\n","num_actions = env.action_space.n\n","\n","# Define the policy network as a PyTorch class\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(PolicyNetwork, self).__init__()\n","        # First fully connected layer with 32 units\n","        self.fc1 = nn.Linear(input_dim, 32)\n","        # ReLU activation function\n","        self.relu = nn.ReLU()\n","        # Output layer to produce logits for each action\n","        self.fc2 = nn.Linear(32, output_dim)\n","        # Softmax layer to convert logits to probabilities\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    # Forward pass through the network\n","    def forward(self, x):\n","        x = self.fc1(x)       # Apply first linear layer\n","        x = self.relu(x)      # Apply ReLU activation\n","        logits = self.fc2(x)  # Apply second linear layer to get logits\n","        probs = self.softmax(logits)  # Convert logits to action probabilities\n","        return probs\n","\n","# Initialize the policy network with state_dim inputs and num_actions outputs\n","policy_net = PolicyNetwork(state_dim, num_actions)\n","\n","# Define the optimizer (Adam) to update the network weights\n","optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n","'''\n","# Function to compute discounted and normalized rewards\n","def compute_discounted_rewards(rewards, gamma=0.99):\n","    discounted = np.zeros_like(rewards, dtype=np.float32)  # Initialize array\n","    running_add = 0  # Initialize the cumulative sum\n","    # Traverse rewards in reverse to apply the discount\n","    for t in reversed(range(len(rewards))):\n","        running_add = running_add * gamma + rewards[t]\n","        discounted[t] = running_add\n","    # Normalize rewards for better training stability\n","    discounted -= np.mean(discounted)\n","    discounted /= (np.std(discounted) + 1e-8)\n","    return discounted\n","'''\n","def compute_reward_to_go(rewards, gamma=0.99):\n","    rtg = np.zeros_like(rewards, dtype=np.float32)\n","    running_add = 0\n","    for t in reversed(range(len(rewards))):\n","        running_add = rewards[t] + gamma * running_add\n","        rtg[t] = running_add\n","    return rtg\n","\n","# Number of training episodes\n","num_episodes = 10\n","\n","# Training loop across episodes\n","for episode in range(num_episodes):\n","    state = env.reset()  # Reset the environment at the beginning of each episode\n","    done = False  # Termination flag for the episode\n","\n","    # Buffers to store the trajectory\n","    episode_states = []\n","    episode_actions = []\n","    episode_rewards = []\n","\n","    # Generate a full episode\n","    while not done:\n","        # Convert the state to a tensor and add a batch dimension\n","        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # shape: [1, state_dim]\n","\n","        # Get action probabilities from the policy network (detach to avoid gradients)\n","        action_probs = policy_net(state_tensor).detach().numpy().ravel()\n","\n","        # Sample an action from the probability distribution\n","        action = np.random.choice(num_actions, p=action_probs)\n","\n","        # Step the environment with the chosen action\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Store the transition\n","        episode_states.append(state)\n","        episode_actions.append(action)\n","        episode_rewards.append(reward)\n","\n","        # Move to the next state\n","        state = next_state\n","\n","    # Compute discounted rewards from the episode\n","    discounted_rewards = compute_discounted_rewards(episode_rewards)\n","    discounted_rewards_tensor = torch.FloatTensor(discounted_rewards)\n","\n","    # Convert episode data to tensors\n","    states_tensor = torch.FloatTensor(episode_states)         # [batch_size, state_dim]\n","    actions_tensor = torch.LongTensor(episode_actions)        # [batch_size]\n","\n","    # Compute action probabilities for the whole episode\n","    action_probs = policy_net(states_tensor)                  # [batch_size, num_actions]\n","\n","    # Gather the probabilities of the actions actually taken\n","    selected_log_probs = torch.log(action_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze())\n","\n","    # Compute the policy loss: -log(pi(a|s)) * reward\n","    loss = -torch.mean(selected_log_probs * discounted_rewards_tensor)\n","\n","    # Backpropagation step\n","    optimizer.zero_grad()  # Clear old gradients\n","    loss.backward()        # Compute new gradients\n","    optimizer.step()       # Update network parameters\n","\n","    # Print progress every 10 episodes\n","    if (episode + 1) % 10 == 0:\n","        print(f\"Episode {episode + 1}, Return: {np.sum(episode_rewards):.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLEvYmkJWhzZ","executionInfo":{"status":"ok","timestamp":1751983689585,"user_tz":-330,"elapsed":87235,"user":{"displayName":"Geetanjali Sharma","userId":"06177037883473308767"}},"outputId":"750b7b68-ea0f-41f2-fe30-69944f67925c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["Episode 10, Return: 21.00\n","Episode 20, Return: 35.00\n","Episode 30, Return: 22.00\n","Episode 40, Return: 117.00\n","Episode 50, Return: 176.00\n","Episode 60, Return: 159.00\n","Episode 70, Return: 344.00\n","Episode 80, Return: 391.00\n","Episode 90, Return: 438.00\n","Episode 100, Return: 179.00\n","Episode 110, Return: 312.00\n","Episode 120, Return: 437.00\n","Episode 130, Return: 399.00\n","Episode 140, Return: 500.00\n","Episode 150, Return: 115.00\n","Episode 160, Return: 494.00\n","Episode 170, Return: 500.00\n","Episode 180, Return: 471.00\n","Episode 190, Return: 500.00\n","Episode 200, Return: 500.00\n","Episode 210, Return: 403.00\n","Episode 220, Return: 500.00\n","Episode 230, Return: 500.00\n","Episode 240, Return: 330.00\n","Episode 250, Return: 161.00\n","Episode 260, Return: 295.00\n","Episode 270, Return: 159.00\n","Episode 280, Return: 500.00\n","Episode 290, Return: 500.00\n","Episode 300, Return: 500.00\n","Episode 310, Return: 500.00\n","Episode 320, Return: 500.00\n","Episode 330, Return: 500.00\n","Episode 340, Return: 500.00\n","Episode 350, Return: 500.00\n","Episode 360, Return: 500.00\n","Episode 370, Return: 500.00\n","Episode 380, Return: 279.00\n","Episode 390, Return: 500.00\n","Episode 400, Return: 500.00\n","Episode 410, Return: 500.00\n","Episode 420, Return: 500.00\n","Episode 430, Return: 500.00\n","Episode 440, Return: 500.00\n","Episode 450, Return: 500.00\n","Episode 460, Return: 500.00\n","Episode 470, Return: 500.00\n","Episode 480, Return: 500.00\n","Episode 490, Return: 117.00\n","Episode 500, Return: 500.00\n","Episode 510, Return: 97.00\n","Episode 520, Return: 108.00\n","Episode 530, Return: 104.00\n","Episode 540, Return: 57.00\n","Episode 550, Return: 146.00\n","Episode 560, Return: 162.00\n","Episode 570, Return: 177.00\n","Episode 580, Return: 131.00\n","Episode 590, Return: 115.00\n","Episode 600, Return: 66.00\n","Episode 610, Return: 38.00\n","Episode 620, Return: 43.00\n","Episode 630, Return: 65.00\n","Episode 640, Return: 127.00\n","Episode 650, Return: 184.00\n","Episode 660, Return: 163.00\n","Episode 670, Return: 330.00\n","Episode 680, Return: 346.00\n","Episode 690, Return: 259.00\n","Episode 700, Return: 213.00\n","Episode 710, Return: 295.00\n","Episode 720, Return: 253.00\n","Episode 730, Return: 334.00\n","Episode 740, Return: 261.00\n","Episode 750, Return: 325.00\n","Episode 760, Return: 413.00\n","Episode 770, Return: 500.00\n","Episode 780, Return: 500.00\n","Episode 790, Return: 500.00\n","Episode 800, Return: 500.00\n","Episode 810, Return: 500.00\n","Episode 820, Return: 500.00\n","Episode 830, Return: 500.00\n","Episode 840, Return: 500.00\n","Episode 850, Return: 500.00\n","Episode 860, Return: 500.00\n","Episode 870, Return: 500.00\n","Episode 880, Return: 500.00\n","Episode 890, Return: 500.00\n","Episode 900, Return: 500.00\n","Episode 910, Return: 500.00\n","Episode 920, Return: 500.00\n","Episode 930, Return: 500.00\n","Episode 940, Return: 500.00\n","Episode 950, Return: 500.00\n","Episode 960, Return: 500.00\n","Episode 970, Return: 500.00\n","Episode 980, Return: 500.00\n","Episode 990, Return: 500.00\n","Episode 1000, Return: 500.00\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_cydoVbJXDey"},"execution_count":null,"outputs":[]}]}